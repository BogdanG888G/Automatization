import os
import re
import logging
import pandas as pd
import pickle
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from airflow import DAG
from airflow.operators.python import PythonOperator

def clean_text(text):
    if pd.isna(text) or not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'[¬´¬ª""‚Äú‚Äù]', '', text)
    text = re.sub(r'[.,;:/\-]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def train_branch_model():
    INPUT_CSV = "ml_models/address_enrichment/labeled_branch_data.csv"  # –ø—É—Ç—å –∫ —Ç–≤–æ–µ–º—É csv
    MODEL_PATH = "ml_models/address_enrichment/branch_model.pkl"
    VECTORIZER_PATH = "ml_models/address_enrichment/branch_vectorizer.pkl"

    logging.info(f"üìÇ –¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")

    if not os.path.exists(INPUT_CSV):
        raise FileNotFoundError(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {INPUT_CSV}")

    df = pd.read_csv(INPUT_CSV, sep=';').drop_duplicates()
    df.dropna(subset=['city', 'branch'], inplace=True)

    logging.info(f"üìä –ö–æ–ª-–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤: {df['city'].nunique()}")
    logging.info(f"üìä –ö–æ–ª-–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª–∏–∞–ª–æ–≤: {df['branch'].nunique()}")

    df['cleaned_city'] = df['city'].apply(clean_text)
    df = df[df['cleaned_city'].str.len() > 1]

    branch_counts = df['branch'].value_counts()
    valid_branches = branch_counts[branch_counts >= 2].index
    df = df[df['branch'].isin(valid_branches)]

    if df.empty:
        raise ValueError("‚ùå –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ñ–∏–ª–∏–∞–ª–∞–º —Å –º–∏–Ω–∏–º—É–º 2 –ø—Ä–∏–º–µ—Ä–∞–º–∏ –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç.")

    logging.info(f"üìä –ö–æ–ª-–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª–∏–∞–ª–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {df['branch'].nunique()}")

    X = df['cleaned_city']
    y = df['branch']

    vectorizer = TfidfVectorizer(
        ngram_range=(1, 3),
        max_features=10000,
        min_df=2,
        max_df=0.95
    )
    X_vec = vectorizer.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X_vec, y, test_size=0.4, random_state=42, stratify=y
    )

    model = LogisticRegression(
        C=1.0,
        max_iter=2000,
        n_jobs=-1,
        random_state=42,
        verbose=1,
        class_weight='balanced'
    )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    logging.info("\nüìà –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ (—Ñ–∏–ª–∏–∞–ª –ø–æ –≥–æ—Ä–æ–¥—É):\n" + classification_report(y_test, y_pred, zero_division=0))

    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
    with open(MODEL_PATH, 'wb') as f_model:
        pickle.dump(model, f_model)
    with open(VECTORIZER_PATH, 'wb') as f_vec:
        pickle.dump(vectorizer, f_vec)

    logging.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Ñ–∏–ª–∏–∞–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODEL_PATH}")
    logging.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {VECTORIZER_PATH}")

# DAG airflow
default_args = {
    'start_date': datetime(2023, 1, 1),
    'owner': 'airflow',
    'retries': 1,
}

with DAG(
    dag_id='train_branch_model_dag',
    default_args=default_args,
    schedule_interval=None,
    max_active_runs=1,
    concurrency=1,
    catchup=False,
    tags=['model_training', 'address', 'branch'],
    description='–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ñ–∏–ª–∏–∞–ª–∞ –ø–æ –∞–¥—Ä–µ—Å—É (city)',
) as dag:

    train_branch_task = PythonOperator(
        task_id='train_branch_model',
        python_callable=train_branch_model,
    )

    train_branch_task
