# üëá –ò–º–ø–æ—Ä—Ç—ã
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import pandas as pd
import pickle
import os
import re
import logging  

from natasha import (
    MorphVocab,
    AddrExtractor
)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# üëá Natasha init
morph_vocab = MorphVocab()
addr_extractor = AddrExtractor(morph_vocab)


def extract_city_with_natasha(text):
    if pd.isna(text) or not isinstance(text, str):
        return None

    # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–µ–≥—É–ª—è—Ä–∫–∞–º–∏
    simple_patterns = [
        r'\b–≥\.\s*([–ê-–Ø–∞-—è–Å—ë-]+)',
        r'\b–≥–æ—Ä–æ–¥\s*([–ê-–Ø–∞-—è–Å—ë-]+)',
        r'\b–≥\s+([–ê-–Ø–∞-—è–Å—ë-]+)',
        r',\s*([–ê-–Ø–∞-—è–Å—ë-]+)\s*,'
    ]
    for pattern in simple_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group(1).strip()

    try:
        matches = addr_extractor(text)
        for match in matches:
            fact_dict = match.fact.as_json
            # –ü—Ä–∏–º–µ—Ä: {'parts': [{'type': 'region', 'value': '–ú–æ—Å–∫–æ–≤—Å–∫–∞—è'}, {'type': 'city', 'value': '–ú–æ—Å–∫–≤–∞'}]}
            for part in fact_dict.get("parts", []):
                if part.get("type") in ("–≥–æ—Ä–æ–¥", "city"):
                    return part.get("value")
    except Exception as e:
        logging.warning(f"–û—à–∏–±–∫–∞ Natasha –¥–ª—è –∞–¥—Ä–µ—Å–∞: {text[:50]}... - {str(e)}")

    return None



def clean_address(text):
    if pd.isna(text) or not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'[¬´¬ª""‚Äú‚Äù]', '', text)
    text = re.sub(r'[.,;:/\-]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def train_city_model():
    INPUT_CSV = "ml_models/address_enrichment/labeled_address_data.csv"
    MODEL_PATH = "ml_models/address_enrichment/city_model.pkl"
    VECTORIZER_PATH = "ml_models/address_enrichment/city_vectorizer.pkl"

    logging.info(f"üìÇ –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")

    if not os.path.exists(INPUT_CSV):
        raise FileNotFoundError(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {INPUT_CSV}")

    try:
        df = pd.read_csv(INPUT_CSV, sep=';').dropna().drop_duplicates().head(10000)
        logging.info(f"üîç –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(df)}")
    except Exception as e:
        raise ValueError(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ CSV: {str(e)}")

    df.dropna(subset=['address', 'city'], inplace=True)

    logging.info("üîç –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–æ—Ä–æ–¥–∞ —Å –ø–æ–º–æ—â—å—é Natasha...")
    df['natasha_city'] = df['address'].apply(extract_city_with_natasha)

    df['natasha_correct'] = df.apply(
        lambda x: str(x['natasha_city']).lower() == str(x['city']).lower()
        if pd.notna(x['natasha_city']) else False,
        axis=1
    )
    accuracy = df['natasha_correct'].mean()
    logging.info(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å Natasha: {accuracy:.2%}")

    if accuracy < 0.7:
        logging.warning("‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –¢–æ—á–Ω–æ—Å—Ç—å Natasha –Ω–∏–∂–µ 70%. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö.")

    mismatches = df[~df['natasha_correct']][['address', 'city', 'natasha_city']].head(10)
    logging.info(f"üî¢ –ü—Ä–∏–º–µ—Ä—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π:\n{mismatches.to_string(index=False)}")

    df['final_city'] = df['natasha_city'].fillna(df['city'])
    df['cleaned_address'] = df['address'].apply(clean_address)
    df = df[df['cleaned_address'].str.len() > 5]

    city_counts = df['final_city'].value_counts()
    df = df[df['final_city'].isin(city_counts[city_counts >= 2].index)]

    if df.empty:
        raise ValueError("‚ùå –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Å—Ç.")

    logging.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤: {df['final_city'].nunique()}")
    logging.info(f"üèôÔ∏è –¢–æ–ø-10 –≥–æ—Ä–æ–¥–æ–≤:\n{df['final_city'].value_counts().head(10)}")

    X = df['cleaned_address']
    y = df['final_city']

    vectorizer = TfidfVectorizer(
        ngram_range=(1, 3),
        max_features=5000,
        min_df=2,
        max_df=0.9
    )
    X_vec = vectorizer.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X_vec, y, test_size=0.4, random_state=42, stratify=y
    )

    model = LogisticRegression(
        C=1.0,
        max_iter=1000,
        n_jobs=-1,
        random_state=42,
        verbose=1,
        class_weight='balanced'
    )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    logging.info("\nüìà –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏:\n" + classification_report(y_test, y_pred, zero_division=0))

    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
    with open(MODEL_PATH, 'wb') as f_model:
        pickle.dump(model, f_model)
    with open(VECTORIZER_PATH, 'wb') as f_vec:
        pickle.dump(vectorizer, f_vec)

    logging.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODEL_PATH}")
    logging.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {VECTORIZER_PATH}")


# DAG
default_args = {
    'start_date': datetime(2023, 1, 1),
    'owner': 'airflow',
    'retries': 1,
}

with DAG(
    dag_id='train_city_model_dag',
    default_args=default_args,
    schedule_interval=None,
    max_active_runs=1,
    concurrency=1,
    catchup=False,
    tags=['model_training', 'address', 'city'],
    description='–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≥–æ—Ä–æ–¥–∞ –∏–∑ –∞–¥—Ä–µ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha',
) as dag:

    train_model_task = PythonOperator(
        task_id='train_city_model',
        python_callable=train_city_model,
    )

    train_model_task
