# üëá –ò–º–ø–æ—Ä—Ç—ã
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import pandas as pd
import pickle
import os
import re
import logging  

from natasha import (
    MorphVocab,
    AddrExtractor
)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

import pandas as pd
import numpy as np
import os
import re
import pickle
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from natasha import MorphVocab, AddrExtractor

# Natasha init
morph_vocab = MorphVocab()
addr_extractor = AddrExtractor(morph_vocab)

def normalize_address(text: str) -> str:
    """–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ –∫ –µ–¥–∏–Ω–æ–º—É –≤–∏–¥—É"""

    if not isinstance(text, str):
        return ""
    text = text.lower()
    replace_dict = {
        r'\b–≥\.\b': '–≥–æ—Ä–æ–¥ ',
        r'\b–≥–æ—Ä\b': '–≥–æ—Ä–æ–¥ ',
        r'\b—É–ª\b': '—É–ª–∏—Ü–∞ ',
        r'\b–ø—Ä-–∫—Ç\b': '–ø—Ä–æ—Å–ø–µ–∫—Ç ',
        r'\b—Ä–µ—Å–ø\b': '—Ä–µ—Å–ø—É–±–ª–∏–∫–∞ ',
    }
    for pattern, repl in replace_dict.items():
        text = re.sub(pattern, repl, text)
    text = re.sub(r'[¬´¬ª"‚Äú‚Äù]', '', text)
    text = re.sub(r'[.,;:/\-]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def extract_natasha_city(text: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ—Ä–æ–¥ —Å –ø–æ–º–æ—â—å—é Natasha"""

    try:
        matches = addr_extractor(text)
        for match in matches:
            for part in match.fact.parts:
                if part.type in ("–≥–æ—Ä–æ–¥", "city"):
                    return part.value.lower()
    except Exception:
        pass
    return ""

def train_city_model():
    INPUT_CSV = "ml_models/address_enrichment/labeled_address_data.csv"
    MODEL_PATH = "ml_models/address_enrichment/city_model.pkl"
    VECTORIZER_PATH = "ml_models/address_enrichment/city_vectorizer.pkl"

    if not os.path.exists(INPUT_CSV):
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {INPUT_CSV}")

    df = pd.read_csv(INPUT_CSV, sep=';').sample(10000).dropna().drop_duplicates()
    
    
    logging.info('–ü—Ä–∏–≤–æ–¥–∏–º –∞–¥—Ä–µ—Å–∞ –∫ –µ–¥–∏–Ω–æ–º—É –≤–∏–¥—É')
    # –û—á–∏—Å—Ç–∫–∞ –∞–¥—Ä–µ—Å–æ–≤
    df['cleaned_address'] = df['address'].apply(normalize_address)
    logging.info('–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–æ—Ä–æ–¥ —Å –ø–æ–º–æ—â—å—é Natasha')
    
    
    # Natasha —Ñ–∏—á–∞
    df['natasha_city'] = df['address'].apply(extract_natasha_city)
    logging.info('–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–∫–æ–Ω—á–µ–Ω–æ')

    # –§–∏–ª—å—Ç—Ä –ø–æ –¥–ª–∏–Ω–µ –∞–¥—Ä–µ—Å–∞
    df = df[df['cleaned_address'].str.len() > 5]

    # –£–±–∏—Ä–∞–µ–º –æ—á–µ–Ω—å —Ä–µ–¥–∫–∏–µ –≥–æ—Ä–æ–¥–∞
    city_counts = df['city'].value_counts()
    df = df[df['city'].isin(city_counts[city_counts >= 3].index)]

    logging.info(f"–û—Å—Ç–∞–ª–æ—Å—å {len(df)} —Å—Ç—Ä–æ–∫, {df['city'].nunique()} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤")

    # –ü—Ä–∏–∑–Ω–∞–∫–∏: —Ç–µ–∫—Å—Ç –∞–¥—Ä–µ—Å–∞ + –≥–æ—Ä–æ–¥ Natasha
    df['full_features'] = df['cleaned_address'] + " natasha:" + df['natasha_city']

    vectorizer = TfidfVectorizer(
        analyzer='char_wb',
        ngram_range=(3, 6),
        max_features=20000
    )
    X_vec = vectorizer.fit_transform(df['full_features'])
    y = df['city']

    X_train, X_test, y_train, y_test = train_test_split(
        X_vec, y, test_size=0.3, random_state=42, stratify=y
    )

    model = LogisticRegression(
        C=3.0,
        max_iter=1500,
        n_jobs=-1,
        class_weight='balanced'
    )
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    logging.info("\n" + classification_report(y_test, y_pred, zero_division=0))

    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
    with open(MODEL_PATH, 'wb') as f_model:
        pickle.dump(model, f_model)
    with open(VECTORIZER_PATH, 'wb') as f_vec:
        pickle.dump(vectorizer, f_vec)

    logging.info("‚úÖ –ú–æ–¥–µ–ª—å –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")


# DAG
default_args = {
    'start_date': datetime(2023, 1, 1),
    'owner': 'airflow',
    'retries': 1,
}

with DAG(
    dag_id='train_city_model_dag',
    default_args=default_args,
    schedule_interval=None,
    max_active_runs=1,
    concurrency=1,
    catchup=False,
    tags=['model_training', 'address', 'city'],
    description='–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≥–æ—Ä–æ–¥–∞ –∏–∑ –∞–¥—Ä–µ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Natasha',
) as dag:

    train_model_task = PythonOperator(
        task_id='train_city_model',
        python_callable=train_city_model,
    )

    train_model_task
